# Logstash Pipeline Configuration v2.0
# Required plugins:
# - logstash-input-beats v8.x
# - logstash-filter-grok v4.x
# - logstash-filter-mutate v3.x
# - logstash-output-elasticsearch v11.x

input {
  # Secure Filebeat input for collecting application logs
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "${SSL_CERT_PATH}"
    ssl_key => "${SSL_KEY_PATH}"
    ssl_verify_mode => "force_peer"
    tags => ["filebeat"]
    add_field => {
      "collector_type" => "filebeat"
      "environment" => "%{[agent][environment]}"
    }
  }

  # Secure TCP input for direct application logging
  tcp {
    port => 5000
    ssl_enable => true
    ssl_cert => "${SSL_CERT_PATH}"
    ssl_key => "${SSL_KEY_PATH}"
    codec => json_lines
    tags => ["direct"]
    add_field => {
      "collector_type" => "tcp"
      "received_at" => "%{@timestamp}"
    }
  }
}

filter {
  # Common timestamp processing
  date {
    match => [
      "timestamp", 
      "ISO8601",
      "UNIX",
      "UNIX_MS",
      "yyyy-MM-dd'T'HH:mm:ss.SSSZ",
      "yyyy-MM-dd HH:mm:ss.SSS"
    ]
    target => "@timestamp"
    timezone => "UTC"
  }

  # Scraper service log processing
  if "scraper" in [tags] {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:scraper_id}\] %{GREEDYDATA:message}",
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} Scraping %{URI:target_url} - %{WORD:status} \(%{NUMBER:duration:float}ms\)"
        ]
      }
      add_field => {
        "service" => "scraper"
      }
    }
    
    # Extract performance metrics
    if [status] == "success" {
      ruby {
        code => '
          event.set("scrape_success", 1)
          event.set("scrape_duration_ms", event.get("duration"))
        '
      }
    }
  }

  # API service log processing
  if "api" in [tags] {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:request_id}\] %{WORD:http_method} %{URIPATH:path} %{NUMBER:status_code:int} %{NUMBER:response_time:float}ms",
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:request_id}\] %{GREEDYDATA:message}"
        ]
      }
      add_field => {
        "service" => "api"
      }
    }

    # Calculate API performance metrics
    if [status_code] and [response_time] {
      ruby {
        code => '
          event.set("api_request_count", 1)
          event.set("api_error_count", event.get("status_code") >= 500 ? 1 : 0)
          event.set("api_response_time_ms", event.get("response_time"))
        '
      }
    }
  }

  # Worker service log processing
  if "worker" in [tags] {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:worker_id}\] Task %{WORD:task_type} - %{WORD:status} \(%{NUMBER:processing_time:float}ms\)",
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:worker_id}\] %{GREEDYDATA:message}"
        ]
      }
      add_field => {
        "service" => "worker"
      }
    }

    # Worker performance metrics
    if [status] and [processing_time] {
      ruby {
        code => '
          event.set("worker_task_count", 1)
          event.set("worker_success_count", event.get("status") == "success" ? 1 : 0)
          event.set("worker_processing_time_ms", event.get("processing_time"))
        '
      }
    }
  }

  # System-wide performance metric processing
  ruby {
    code => '
      if event.get("service")
        event.set("log_timestamp", event.get("@timestamp"))
        event.set("processing_timestamp", Time.now.utc)
        event.set("processing_lag_ms", ((Time.now.utc - event.get("@timestamp")) * 1000).to_i)
      end
    '
  }

  # Clean up and standardize fields
  mutate {
    convert => {
      "status_code" => "integer"
      "response_time" => "float"
      "processing_time" => "float"
      "duration" => "float"
    }
    remove_field => ["message"] if [parsed_message]
    rename => {
      "parsed_message" => "message"
    }
  }
}

output {
  # Secure Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "tech-transfer-logs-%{+YYYY.MM.dd}"
    user => "${ELASTIC_USERNAME}"
    password => "${ELASTIC_PASSWORD}"
    ssl => true
    ssl_certificate_verification => true
    ssl_certificate_authorities => ["${SSL_CA_PATH}"]
    
    # Index template settings
    template_name => "tech-transfer"
    template_overwrite => true
    
    # Performance optimization
    bulk_max_size => 5000
    flush_size => 1000
    idle_flush_time => 1
    
    # Retry configuration
    retry_initial_interval => 2
    retry_max_interval => 64
    retry_on_conflict => 3
    
    ilm_enabled => true
    ilm_rollover_alias => "tech-transfer"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "tech-transfer-policy"
  }
}